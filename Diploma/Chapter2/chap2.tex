% Chapter 2:
\section{Literature survey}\label{chap:chap_2}

%-----------
% Section 1: Brief introduction to autonomous driving and it's modules
\subsection{Criticality}\label{sec:subsec_2.1}

One small branch that can be applied to any type of learning which is not explored deep enough and not widely applied yet is criticality. The criticality has varios ways to be computed when used in reinforcement learning and autonomous driving. Despite that the definition of criticality is general for all the possible implementation fields: the higher vallue of criticality corresponce to the lower value of safety and higher threat level\cite{CriticalityAutonomous2023} in a chosen situation. 

In a field of autonomous driving the knowledge of criticality, especially the ability of the machine to approximate this value, is crusial for the algorithms that are either already used or are still under development. The ability of models that control driving behavior to process the environment, observations and disturbances to evaluate the measure for the threat level of the situation in a real-time can boost the performance and increase the reliability of autonomous driving technologies. 

The policy can be a base to calculate necessary value. The criticality of a state can be accessed throught the comparison of an entropy of the policy's output action distribution captured at a state \texttt{s} with the threshold value. In this method the threshold value \texttt{t} is the one controlling the number of states to be considered "critical"~\cite{huang2018establishingappropriatetrustcritical}: 

\[ 
C_{\pi} = \{ s \mid \mathcal{H}(\pi(\cdot \mid s)) < t \}
\]


Instead of a policy, in actor-critic methods it can be useful to consider the the action-value funciton(Q-function). Mainly the formula decides if the state is critical according to following condition: if the result produced by the random decision-making process is much worse than the result of acting optimally, then the state is critical.

\[
C_{\pi} = \{ s \mid \max_{a} Q^{\pi}(s, a) - \frac{1}{|A|} \sum_{a} Q^{\pi}(s, a) > t \}
\]


There are many other numerical methods to define the criticality value to be used in different algorithms and systems, however, for this thesis there was a simpler solution chosen. The reliable indicator of the criticality is a variance of the Q-function~\cite{Spielberg_Azaria_2022}. The higher the variance of the Q-function among all pollible actions in a particular state, the more one action/s is probable to be selected by the trained policy than rest. The state, where the probability of one state/s is reasonably higher than others can then be labeled "critical".

Considering the existing research into different methods of calculation, evaluation or approximation of criticality the target of this bachelor work was chosen not to develop a more complex, optimised or effective technique to measure criticaliy, not at all. Current research tries to go deeper into applications of criticality and, more precisely critical states, in transfer learning. 

%-----------
% Section 2: 
\subsection{Transfer Learning}\label{sec:subsec_2.2}

Training a machine learning algorithm of an optimal quality and performance requires big sets of training and evaluation data. Those sets are usually hard to gather or generate, as it costs time and money. The approach to be used as a better means of data collection is Transer Learning. The philosophy of this approach is to utilize models trained for tasks related to the target challenge so that the solution does not need to be developed from scratch~\cite{Hosna2022}. Transfer learning tends help the Machine learning to become more universal and adaptive to new tasks as it creates models which own a collection of knowledge of other models. 

The approach of Transfer Learning can easily be found in real life. For instance, in linguistics, if a person has knowledge of one language, it becomes easier for them to learn a new language from the same language group. In the scope of this thesis, transfer learning was performed between two environments: \texttt{Highway-Env} and \texttt{Merge-Env}. 

The necessity and privilege of using Transfer Learning can be seen when applying the approach to tasks related to one another. The autonomous driving task of the agent is to be equally applicable to both environments used throughout the thesis. Moreover, there are some feature sets that those environments share, such as:
\begin{itemize}
    \item Operating between multiple lanes,
    \item Surrounding vehicles capable of accelerating/decelerating and changing lanes.
\end{itemize}

Looking into the scientific field, an analogy can represent the exact approach of Transfer Learning: studying math and physics. If a person who studied math starts deepening their knowledge in physics, this can be called Transfer Learning, as the math expertise makes it easier to acquire new knowledge in the field of physics. However, some themes in math will not be required while learning physics. Compared to the learning process without prior math knowledge, the time and effort investments are noticeably less when Transfer Learning is applied.

% New sub-section:
% ----------------
\subsubsection{Q-Learning}\label{ssec:subsubsec_2.2.1}




