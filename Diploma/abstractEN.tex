%----------Abstract-----------------------------------------------------------
\addsec{Abstract}

While Reinforcement Learning is showing extensive potential in applications which include autonomous driving, its major drawback is the gigantic computing resources and time necessary to train a model from scratch. As a solution, transfer learning was introduced, reducing required data and time when adapting pre-trained models to new environments. However, huge sets of randomly sampled data are traditionally used in transfer learning, introducing great inefficiencies. This research proposes an approach to optimise the standard transfer learning process by leveraging criticality--a measure of the significance of one/more actions in a given state. The training time is reduced when the replay buffer of a model is fed with only critical states during the retraining process while the achieved model performance is maintained comparable to conventional transfer learning. The approach was validated by applying Deep Q-Network (DQN) in \emph{Highway-Env} and \emph{Merge-Env} environments for simulated autonomous driving. Results demonstrate that safety and other performance metrics are evaluated to be comparable for transfer learning based on criticality, although requiring only a fraction of training data. This suggests criticality approach is more feasible for applications in the real world. Insights gathered throughout the thesis will endow the research in the optimisation of machine learning and the development of autonomous driving.